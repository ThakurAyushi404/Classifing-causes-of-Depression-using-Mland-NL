import pandas as pd

# âœ… Load the dataset (update the file path)
file_path = "D:/unitec/MachineLearningCourse/Thesis_code/synthetic_tweets.csv" 
df = pd.read_csv(file_path, dtype=str, low_memory=False)

# âœ… Remove duplicate rows based on the "tweet" column
df_cleaned = df.drop_duplicates(subset=['tweet'], keep='first')

# âœ… Save the cleaned dataset
output_path = "cleaned_dataset.csv"
df_cleaned.to_csv(output_path, index=False)

print(f"\nâœ… Duplicate rows removed! Cleaned dataset saved as '{output_path}'")
print(f"\nðŸ“Œ Original dataset size: {len(df)} rows")
print(f"ðŸ“Œ Cleaned dataset size: {len(df_cleaned)} rows")
